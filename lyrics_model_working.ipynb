{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/maciejglazek/Desktop/UNI/Python/rap_lyrics/lyrics/'\n",
    "\n",
    "'''for filename in os.listdir(directory):\n",
    "    print(filename)'''\n",
    "    \n",
    "lyrics = []\n",
    "for filename in os.listdir(directory):\n",
    "    try:   \n",
    "        with open(directory + filename) as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                lyrics.append(data[\"Lyrics\"])\n",
    "            except: continue\n",
    "    except: continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_lyrics = []\n",
    "for song in lyrics:\n",
    "    for verse in song:\n",
    "        for word in verse:\n",
    "            for letter in word:\n",
    "                flat_lyrics.append(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4110005"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unique characters in the file\n",
    "good = ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_lyrics_clean = []\n",
    "for letter in flat_lyrics: \n",
    "    if letter in good: flat_lyrics_clean.append(letter)\n",
    "    else: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4102521"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_lyrics_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 unique characters\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(flat_lyrics_clean))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in flat_lyrics_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '\"' :   3,\n",
      "  '#' :   4,\n",
      "  '$' :   5,\n",
      "  '%' :   6,\n",
      "  '&' :   7,\n",
      "  \"'\" :   8,\n",
      "  '(' :   9,\n",
      "  ')' :  10,\n",
      "  '*' :  11,\n",
      "  '+' :  12,\n",
      "  ',' :  13,\n",
      "  '-' :  14,\n",
      "  '.' :  15,\n",
      "  '/' :  16,\n",
      "  '0' :  17,\n",
      "  '1' :  18,\n",
      "  '2' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'V', 'e', 'r', 's', 'e', ' ', '1', ':', ' ', 'I', 's', 'a'] ---- characters mapped to int ---- > [59 54 65 78 79 65  1 18 27  1 41 79 61]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(flat_lyrics_clean[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "V\n",
      "e\n",
      "r\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 350\n",
    "examples_per_epoch = len(flat_lyrics_clean)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[Verse 1: Isaiah Rashad]\\nI take a drag at the square, I feel anxious, spit dangerous\\nAs the verbal appears, it's reflectin' my perspective\\nBeer goggle and fear, role model so hollow\\nShadow adolescence and a gaggle of them bitches\\nRoad to the rich is still paved with the ditches\\nGet caught up in the hype, your career is for a night\\nI ain't these othe\"\n",
      "\"r niggas, 'cause these niggas is fake\\nThey ain't poppin' shot, these niggas is Papa Doc\\nFine as the shine, beam me up, and I'm ridin' Spock\\nNo cryin' 'til I'm in a pine box\\nI got the bitches hittin' Mikey like where Zay at?\\nProlly fuckin' on her friend, prolly L.A. at\\nProlly plottin' on a mil', stayin' well fed\\nBitch, don't run your mouth, we don't \"\n",
      "\"play that\\nMomma taught me better, can't count on niggas\\nCan't count on weather  go figure, nigga\\nThey ain't worried 'bout this cheddar\\nThem niggas just sky divin' in my propellers, I don't feel 'em\\nAin't tryna be no freshman, I'm chillin'\\n'Cause they'll bring a knife to the fight, and I'll kill 'em\\nMe and my niggas is hungry, we're willin'\\nTo bring \"\n",
      "\"a little ho to the party and Meek Mill 'em\\nYeah, I feel like talkin' my shit\\nCome in the house, actin' friendly, you can suck my dick\\nI'm prayin' for some good rappers and a Fleetwood\\nWhy I gotta bring my A game? You just D goods\\nWe don't wanna hear that weak shit, nigga, speak up\\nRockin' old flows, cornrows, and a beeper\\nYou niggas hoes, you niggas\"\n",
      "\" hoes, and I ain't even trippin'\\n'Cause we'll be laughin' to the bank and ridin' by these niggas\\nMikey, bring that mink with your low body Bentley\\nPut the hood in the fuckin' trunk and bring the party with me\\n\\n[Chorus: Isaiah Rashad & Jay Rock]\\nI came, I saw, I conquered, ayy  I shot you down\\nNow your brain no have no conscious, ayy  what you do now\"\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"[Verse 1: Isaiah Rashad]\\nI take a drag at the square, I feel anxious, spit dangerous\\nAs the verbal appears, it's reflectin' my perspective\\nBeer goggle and fear, role model so hollow\\nShadow adolescence and a gaggle of them bitches\\nRoad to the rich is still paved with the ditches\\nGet caught up in the hype, your career is for a night\\nI ain't these oth\"\n",
      "Target data: \"Verse 1: Isaiah Rashad]\\nI take a drag at the square, I feel anxious, spit dangerous\\nAs the verbal appears, it's reflectin' my perspective\\nBeer goggle and fear, role model so hollow\\nShadow adolescence and a gaggle of them bitches\\nRoad to the rich is still paved with the ditches\\nGet caught up in the hype, your career is for a night\\nI ain't these othe\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 59 ('[')\n",
      "  expected output: 54 ('V')\n",
      "Step    1\n",
      "  input: 54 ('V')\n",
      "  expected output: 65 ('e')\n",
      "Step    2\n",
      "  input: 65 ('e')\n",
      "  expected output: 78 ('r')\n",
      "Step    3\n",
      "  input: 78 ('r')\n",
      "  expected output: 79 ('s')\n",
      "Step    4\n",
      "  input: 79 ('s')\n",
      "  expected output: 65 ('e')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 350), (64, 350)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 350, 87) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           22272     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 87)            89175     \n",
      "=================================================================\n",
      "Total params: 4,049,751\n",
      "Trainable params: 4,049,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 37,  9, 33, 59, 45, 47, 25, 34, 36, 78, 66, 40, 82, 62, 56, 68,\n",
       "       40, 39, 29, 14, 40, 80, 63, 62, 64, 81, 68, 77, 75, 84, 86, 45, 39,\n",
       "       18, 65, 84, 74, 58, 25, 84,  0, 39, 51,  6, 65, 45,  3, 11, 29, 84,\n",
       "        0, 63, 69, 74, 24, 24, 85, 20, 31, 64, 85, 71, 15, 84, 75, 50, 62,\n",
       "       55,  0, 61, 73, 77,  8, 60, 20, 51, 23, 22, 35, 55, 81, 52,  1, 76,\n",
       "       41, 58, 51, 67, 48,  8, 39, 37, 31, 34, 63, 79, 61, 47, 47, 64, 23,\n",
       "       47, 26, 15, 19, 28, 31,  5, 41, 73, 31, 86, 73, 67, 54, 86, 67, 15,\n",
       "       26, 29, 52, 19, 17, 13, 31, 12, 28, 81, 52, 61, 70,  1, 15, 62, 37,\n",
       "       86,  2, 48, 61, 45, 59, 86,  0, 37, 25, 63, 19,  6, 79, 17, 19, 32,\n",
       "       40, 36,  9, 60, 20, 84, 59, 53, 47, 81, 83, 18, 21, 13, 54, 69, 74,\n",
       "       42, 21, 41, 56, 38, 28, 21, 17, 20,  3, 23, 23, 78, 10, 66,  1, 39,\n",
       "       83, 42, 59, 32,  0, 18, 54,  2, 71, 42, 31, 32, 74,  4, 52, 71,  2,\n",
       "       38, 66, 62, 45, 22, 53, 50, 73, 69, 50, 23, 84, 38, 46, 36,  0, 79,\n",
       "       32,  8,  4, 60, 84, 54, 19, 44, 58, 27, 46, 37, 74, 26, 83,  5, 33,\n",
       "       51, 17, 35, 79, 61, 44, 71, 83,  0, 75, 15, 86, 68, 84,  4, 62, 44,\n",
       "       58, 20, 38, 57, 10, 54, 55, 20, 67, 50, 27, 38, 17, 36, 40, 55,  5,\n",
       "       22, 29, 51, 37, 39, 41, 13, 25, 79, 29, 29, 55, 33, 56, 50, 16, 72,\n",
       "       26, 74, 85, 16, 23, 16, 42, 36, 49, 21, 44, 68, 11, 67, 29, 84, 46,\n",
       "       36, 62,  9, 77, 77, 68, 24, 28, 74, 59, 14, 14, 82, 40, 82, 77,  6,\n",
       "       80, 14, 75, 43,  5, 30, 29, 53, 29, 41,  6, 84, 36, 58, 50, 28, 66,\n",
       "        8, 77, 14,  4, 39, 69,  4, 76,  7, 55])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"ause they told you to, why you listen to 'em?\\nHands up, middle finger to 'em\\nFuck that, get down\\n\\n[Hook]\\nWhen I get down on my luck I hide behind my eyes in Hollywood\\nThey say ain't what you know but who you know\\nYou need to know someone to know no one\\nWhen I get down on my luck\\nI roll one up and roll around all by my lonesome\\nLost some years, I us\"\n",
      "\n",
      "Next Char Predictions: \n",
      " '9E(A[MO8BDrfHvbXhHG<-HtcbduhqoxzMG1exnZ8x\\nGS%eM\"*<x\\ncin77y3>dyk.xoRbW\\namq\\']3S65CWuT pIZSgP\\'GE>BcsaOOd6O9.2;>$Im>zmgVzg.9<T20,>+;uTaj .bEz!PaM[z\\nE8c2%s02?HD(]3x[UOuw14,VinJ4IXF;403\"66r)f GwJ[?\\n1V!kJ>?n#Tk!FfbM5URmiR6xFND\\ns?\\'#]xV2LZ:NEn9w$AS0CsaLkw\\no.zhx#bLZ3FY)VW3gR:F0DHW$5<SEGI,8s<<WAXR/l9ny/6/JDQ4Lh*g<xNDb(qqh7;n[--vHvq%t-oK$=<U<I%xDZR;f\\'q-#Gi#p&W'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 350, 87)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.4658957\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 182 steps\n",
      "Epoch 1/10\n",
      "182/182 [==============================] - 677s 4s/step - loss: 2.6715\n",
      "Epoch 2/10\n",
      "182/182 [==============================] - 676s 4s/step - loss: 1.8750\n",
      "Epoch 3/10\n",
      "182/182 [==============================] - 674s 4s/step - loss: 1.6219\n",
      "Epoch 4/10\n",
      "182/182 [==============================] - 668s 4s/step - loss: 1.4873\n",
      "Epoch 5/10\n",
      "182/182 [==============================] - 700s 4s/step - loss: 1.4017\n",
      "Epoch 6/10\n",
      "182/182 [==============================] - 674s 4s/step - loss: 1.3397\n",
      "Epoch 7/10\n",
      "182/182 [==============================] - 677s 4s/step - loss: 1.2890\n",
      "Epoch 8/10\n",
      "182/182 [==============================] - 671s 4s/step - loss: 1.2454\n",
      "Epoch 9/10\n",
      "182/182 [==============================] - 671s 4s/step - loss: 1.2059\n",
      "Epoch 10/10\n",
      "182/182 [==============================] - 671s 4s/step - loss: 1.1688\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            22272     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 87)             89175     \n",
      "=================================================================\n",
      "Total params: 4,049,751\n",
      "Trainable params: 4,049,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1: Weven be such a stripper\n",
      "Foreign, now that nigga ain't even gave\n",
      "Make you got that weight and I said that it's fuckin' back to me, ho\n",
      "This girl am I badgett a dollarry I'm a livin' in Dod (Uh), yeah, got my life, I\n",
      "I might need to get feelin' me\n",
      "Fucked up like a kison, worly wit' that thing gow\n",
      "\n",
      "[Chorus 1: Villered Sends clappin'\n",
      "No such thing ever done, feel the rain\n",
      "'Cause I-aint done and don't do anything that you walk up with it\n",
      "Man I bet you felt this faggot a city then I'm soardin'\n",
      "Send the ground, put 'em in with those\n",
      "No more big-and like a verse\n",
      "Full of girl got the number and what\n",
      "They started really deepun it when I'm gone, they bustin' at my chillage\n",
      "Beating depending white Busen started than blams\n",
      "In dollars of me, one fun\n",
      "And when she don't got bars like that\n",
      "What them lie like a movie nigga\n",
      "That's feelin' forward when I'm from pay his nigga\n",
      "Let it back on tom of the\n",
      "Im just tryna view this much way before this sauce\n",
      "Tell him, \"Don't think that you thrown!\"\n",
      "The facret fee\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Text1: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
